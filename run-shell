#!/bin/env bash
# ============================================================
#
# Run spark shell template 
#
# ============================================================

# set the default spark/hadoop conf directories
SPARK_SPECULATION=false
if grep -i -q ulviasp00949 <<<$(hostname); then
  SPARK_HOME=/app/spark-2.1.0-bin-hadoop2.7
elif grep -i -q ulvuq2p02 <<<$(hostname); then
  SPARK_HOME=/opt/spark-2.0.2-bin-hadoop2.7
fi
SPARK_CONF_DIR=/etc/titan/spark
HADOOP_CONF_DIR=/etc/titan/hadoop
QUEUE=PDigital

NUM_EXECUTORS=32
NUM_EXECUTOR_CORES=4
DEFAULT_PARALLELISM=$((4 * NUM_EXECUTOR_CORES * NUM_EXECUTORS))

# search the local lib/ directory for jars
JAR_DIR=lib
JARS="
  $JAR_DIR/graphframes-0.5.0-spark2.1-s_2.11.jar,
  $JAR_DIR/scala-logging-api_2.11-2.1.2.jar,
  $JAR_DIR/scala-logging-slf4j_2.11-2.1.2.jar,
  $JAR_DIR/spark-avro_2.11-3.2.0.jar,
  $JAR_DIR/breeze_2.11-0.13.2.jar,
  $JAR_DIR/lamma_2.11-2.3.1.jar,
  $JAR_DIR/scala-csv_2.11-1.3.4.jar,
  $(find target -name '*.jar' | tail -n 1)
"
spark-shell \
  --jars $(tr -d ' \n\t' <<<$JARS) \
  --master yarn \
  --deploy-mode client \
  --num-executors $NUM_EXECUTORS \
  --executor-cores $NUM_EXECUTOR_CORES \
  --executor-memory 32G \
  --conf spark.default.parallelism=$DEFAULT_PARALLELISM \
  --conf spark.yarn.executor.memoryOverhead=4G \
  --conf spark.sql.autoBroadcastJoinThreshold=-1 \
  --conf spark.sql.crossJoin.enabled=true \
  --conf spark.sql.pivotMaxValues=100000 \
  --conf spark.sql.shuffle.partitions=2048 \
  --conf spark.speculation=$SPARK_SPECULATION \
  --conf spark.speculation.interval=100ms \
  --conf spark.speculation.multiplier=1.5 \
  --conf spark.speculation.quantile=0.75 \
  --queue $QUEUE $@

  # --conf spark.network.timeout=9600 \
  # --conf spark.reducer.maxBlocksInFlightPerAddress=512 \
  # --conf spark.driver.maxResultSize=6G \
  # --conf spark.yarn.am.memory=1G \
  # --conf spark.maxRemoteBlockSizeFetchToMem=264M \
  # --conf spark.yarn.scheduler.heartbeat.interval-ms=720000 \
  # --conf spark.executor.heartbeatInterval=720000 \
  # --conf spark.network.timeout=720000 \
